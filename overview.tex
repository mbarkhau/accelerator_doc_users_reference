This chapter presents an overview of the Accelerator's features in a
rather non-formal way.

\section{High Level View}

The Accelerator is a client-server based application, and from a high
level, it can be visualised like in figure~\ref{fig:overview}.
\begin{figure}[h!]
  \begin{center}
    \input{figures/overview.pdftex_t}
    \caption{High level view of the Accelerator framework.  See text
      for details.}
    \label{fig:overview}
  \end{center}
\end{figure}
This manual will describe the setup in detail.  For now, the most
important features are as follows.  On the left side there is a
\texttt{runner} client.  To the right, there are two servers, called
\texttt{daemon} and \texttt{urd}.  The \texttt{runner} program runs
scripts, called \texttt{build scripts}, that execute jobs on the
\texttt{daemon} server.  This server will load and store information
and results for all jobs executed using the \textsl{workdirs} file
system based database.  In parallel, all jobs covered by a build
script will be stored by the \texttt{urd} server into the \textsl{job
  logs} file system database.  \texttt{urd} is also responsible for
finding collections, or lists, of related previously executed jobs.



\section{Jobs:  Executing Code}
The basic operation of the Accelerator is to execute small programs
called \textsl{methods}.  In a method, some reserved function names
are used to execute code sequentially or in parallel and to pass
parameters and results.  A method that has completed execution is
called a \textsl{job}.

The Accelerator keeps a record of all jobs that have been run.  It
turns out this is very useful for avoiding unnecessary re-computing
and instead rely on previosly computed results.  This does not only
speed up processing and encourage incremental design, but also makes
it transparent which code and which data was used for any particular
result, thus reducing uncertainty.

In this section, we'll look at basic job running, result sharing, and
parameters.


\subsection{Basic Job Running:  ``Hello, World''}

Let's begin with a simple ``hello world'' program.  We create a method
with the following contents
\begin{python}
def synthesis():
    return "hello world"
\end{python}
This program does not take any input parameters.  It just returns a
string and exits.  Running methods on the Accelerator is further
explained in section~\ref{chap:urd_basic}.

When execution of the method is completed, a single link, called a
\texttt{jobid} is the only thing that is returned to the user.  The
jobid points to a directory where the result from the execution is
stored, together with all information that was needed to run the job
plus some profiling information.

If we try to run the job again it will not execute, simply because the
Accelerator remembers the job has been run in the past.  Instead of
running the job again, it immediately returns the jobid pointing to
the previous run.  This means that from a user's perspective, there is
no difference between job running and job result recalling!  In order
to have the job executing again, we have to change either the source
code or input parameters.

Figure~\ref{fig:execflow-hello-world} illustrates the dispatch of the
\texttt{hello\_world} method.  The created jobid is called
\texttt{test-0}, and corresponding directory information is shown in
green.  The job directory contains several files, of which the most
important ones right now are
\begin{itemize}
\item[] \texttt{setup.json}, which contains job information; and
\item[] \texttt{result.pickle}, that contains the returned data.
\end{itemize}

\begin{figure}[h!]
  \begin{center}
    \input{figures/job0.pdftex_t}
    \caption{A simple hello world program, represented as graph and
      work directory.}
    \label{fig:execflow-hello-world}
  \end{center}
\end{figure}






\subsection{Linking Jobs}
Assume that the job that we just run was computationally expensive,
and that it returned a result that we'd like to use as input to further
processing.

To keep things simple, we demonstrate the principle by creating a
method that just reads and prints the result from the previous job to
\texttt{stdout}.  We create a new method \texttt{print\_result}, that
goes like this

\begin{python}
import blob

jobids = {'hello_world_job',}

def synthesis():
    x = blob.load(jobid=jobids.hello_world_job)
    print(x)
\end{python}

This method expects the \texttt{hello\_world\_job} input parameter to
be provided at execution time, and we will see in section~\ref{sec:jobparams}
how to do this.  The method then reads the result from the provided
jobid and assigns it to the variable \texttt{x}, which is then printed
to \texttt{stdout}.  Note that this method does not return anything.
Figure~\ref{fig:execflow-print-result} illustrates the situation.
Note the direction of the arrow - The second job, \texttt{test-1} had
\texttt{test-0} as input parameter, but \texttt{test-0} does not know
of any jobs run in the future.

\begin{figure}[h!]
  \begin{center}
    \input{figures/job0job1.pdftex_t}
    \caption{Jobid \texttt{test-0}, is used as input to the
      \texttt{print\_result} job.}
    \label{fig:execflow-print-result}
  \end{center}
\end{figure}

\subsection{Job Execution Flow and Result Passing}

There are three functions in a method that are called from the
Accelerator when a method is running, and they are \texttt{prepare()},
\texttt{analysis()}, and \texttt{synthesis()}.  All three may exist in
the same method, and at least one is required.  When the method
executes, they are called one after the other.
\begin{itemize}
\item[] \texttt{prepare()} is executed first.  The returned value is
  available in the variable \texttt{prepare\_res}.
\item[] \texttt{analysis()} is run in parallel processes, one for each
  slice.  It is called after completion of \texttt{prepare()}.  Common
  input paramerers are \texttt{sliceno}, holding the number of the
  current process instance, and \texttt{prepare\_res}.  The return
  value for each process becomes available in the
  \texttt{analysis\_res} variable.
\item[] \texttt{synthesis()} is called after the last
  \texttt{analysis()}-process is completed.  It is typically used to
  aggregate parallel results created by \texttt{analysis()} and takes
  both \texttt{prepare\_res} and \texttt{analysis\_res} as optional
  parameters.  The latter is an iterator of the results from the
  parallel processes.
\end{itemize}
Figure~\ref{fig:prepanasyn} shows the execution order from top to
bottom, and the data passed between functions in coloured branches.
\texttt{prepare()} is executed first, and its return value is
available to both the \texttt{analysis()} and \texttt{synthesis()}
functions.  There are \texttt{slices} (a configurable parameter)
number of parallel \texttt{analysis()} processes, and their output is
available to the \texttt{synthesis()} function, which is executed
last.

Return values from any of the three functions may be stored in the
job's directory making them available to other jobs.


\begin{figure}[h!]
  \begin{center}
    \input{figures/prepanasyn.pdftex_t}
    \caption{Execution flow and result propagation in a method.}
    \label{fig:prepanasyn}
  \end{center}
\end{figure}



\subsection{Job Parameters}
\label{sec:jobparams}
We've seen how jobids from completed jobs can be used as input to new
jobs.  Jobid parameters is one of three kinds of input parameters that
a job can take.  Here the input parameters are summarised:
\begin{itemize}
\item[] \texttt{jobids}, a set of identifiers to previously executed jobs;
\item[] \texttt{options}, a dictionary of options; and
\item[] \texttt{datasets}, a set of input \textsl{datasets}, explained later.
\end{itemize}
See Figure~\ref{fig:execflow}.  Parameters are entered as global
variables early in the method's source.



\subsubsection*{The Params Variable}
A job's parameters are always available in the \texttt{params}
variable.  The \texttt{params} variable is make available by adding it
as input to \texttt{prepare()}, \texttt{analysis()}, or
\texttt{synthesis()}, like this
\begin{python}
def synthesis(params):
    print(params)
\end{python}
Accessing the \texttt{params} variable for \textsl{another} job is
done like this
\begin{python}
jobids = {'thejob',}

def synthesis():
    print(jobids.thejob.params)
\end{python}
it turns out it can be very useful to know for example which datasets
another job has been processing, and so on.

Apart from the input parameters, the \texttt{params} variable also
gives access to more information about the job and the current
Accelerator setup, such as the total number of slices, the random
seed, the hash of the source code, and method execution start time.


\begin{figure}[h!]
  \begin{center}
    \input{figures/execflow.pdftex_t}
    \caption{Execution flow of a method.  The method takes optionally
      three kinds of parameters: \texttt{options}, \texttt{jobids},
      and \texttt{datasets}.}
    \label{fig:execflow}
  \end{center}
\end{figure}


\subsubsection*{Input parameter \texttt{jobids}}
The \texttt{jobids} parameter is a set containing any number of input
jobids.  For example
\begin{python}
jobids = {'sales_stats', 'overhead_stats',}
\end{python}
When a method is to be executed, actual links to jobs, i.e.\ jobids,
are passed using these parameters.


\subsubsection*{Input parameter \texttt{options}}
Options are supplied as a dictionary and is very flexible in typing.  For example
\begin{python}
options = dict(
    name = 'alice',
    defaultnames = set(['alice', 'bob']),
    param1 = 1,
    param2 = float,
)
\end{python}
Here, values are defaults, so \texttt{param1} is default set to 1,
while a floating point value must be supplied to \texttt{param2}.

\subsubsection*{Input parameter \texttt{datasets}}
Datasets will be described in more details later, and the
\texttt{datasets} parameter is similar to the \texttt{jobids}
parameter.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Datasets: Storing Data}

The \texttt{dataset} is the Accelerator's default storage type for
small or large quantities of data, designed for parallel processing
and high performance.  Datasets are built on top of jobs, so
\emph{datasets are created by methods and stored in job directories,
  just like any job result.}

Internally, data in a dataset is stored in a row-column format, and is
typically \emph{sliced} into a fixed number of slices to allow
efficient parallell access.  Columns are accessed independently.
Furthermore, datasets may be \textsl{hashed}, so that slicing is based
on the hash value of a given column.  In many practical applications,
hashing makes parallel processes independent, minimising the need for
complicated merging operations.  This is explained further in
chapter~\ref{chap:datasets}.



\subsection{Importing Data}

Let's have a look at the common operation of \textsl{importing},
i.e.\ creating a dataset from a file.  See
figure~\ref{fig:dataset_csvimport}.

\begin{figure}[h!]
  \begin{center}
    \input{figures/import_file1.pdftex_t}
    \caption{Importing \texttt{file0.txt}.}
    \label{fig:dataset_csvimport}
  \end{center}
\end{figure}

\noindent The standard method \texttt{csvimport}, is designed to parse
a pletoria of ``comma separated values''-file formats and store the
data as a dataset.  The method takes several input options, where the
file name is mandatory.  The created dataset is stored in the
resulting job, and the name of the dataset will by default be the
jobid plus the string \texttt{default}.  For example, if the
\texttt{csvimport} jobid is \texttt{imp-0}, the dataset will be
referenced by \texttt{imp-0/default}.  In this case, and always when
there is no ambiguity, the jobid alone (\texttt{imp-0}) could be used
too.





\subsection{Linking Datasets, Chaining}

Just like jobs can be linked to each other, datasets can link to
each other too.  Since datasets are build on top of jobs, this is
straightforward.  Assume that we've just imported \texttt{file0.txt}
into \texttt{imp-0/default} and that there is more data like it stored
in \texttt{file1.txt}.  We can import the latter file and supply a
link to the previous dataset, see
figure~\ref{fig:dataset_csvimport_chain}.

\begin{figure}[h!]
  \begin{center}
    \input{figures/import_file0file1.pdftex_t}
    \caption{Chaining the import of \texttt{file1.txt} to the previous
      import of \texttt{file0.txt}.}
    \label{fig:dataset_csvimport_chain}
  \end{center}
\end{figure}

\noindent The \texttt{imp-1} (or \texttt{imp-1/default}) dataset
reference can now be used to access all data imported from both files!

Linking datasets containing related content is called \emph{chaining},
and this is particularly convenient when dealing with data that grows
over time.  Good example are all kind of \emph{log} data, such as logs
of transactions, user interactions, etc.

Using chaining, we can extend datasets with more rows just by linking,
which is a very lightweight operation.



\subsection{Adding New Columns to a Dataset}
We have seen how easy it is to add more lines to data to a dataset
using chaining.  The only thing that needs to be done is to set a
link, so the overhead is minimal.

Now we'll see that is it equally simple to add new columns to an
existing dataset.  Adding columns is also a common operation and the
Accelerator handles it efficiently using links.

The idea is very simple.  Assume that we have a ``source'' dataset to
which we want to add a new column.  We create a new dataset containing
\textsl{only} the new column, and while creating it we instruct the
Accelerator to link the source dataset to the new column.

Accessing the new one-column dataset will transparently access all the
data in the source dataset too, making it indistinguishable from a
single dataset.  See Figure~\ref{fig:dep_dataset_append_column}.

\begin{figure}[h!]
  \begin{center}
    \input{figures/dataset_append_column.pdftex_t}
    \caption{caption here}
    \label{fig:dep_dataset_append_column}
  \end{center}
\end{figure}





\subsection{Multiple Datasets in a Job}

We've seen that datasets are created by methods and stored in job
directories.  Typically, a method creates a single dataset in the job
directory, but there is no limit on how many datasets that could be
created and stored in a single job directory.  This leads to some
interesting applications.

One application where it is convenient to create multiple datasets in
a job is when splitting data into subsets based on some condition.
For example, assume that we want to separate a dataset into two
disjoint datasets based on a column storing a boolean value.  See
Figure~\ref{fig:dep_dataset_csvimport_chain}.

\begin{figure}[h!]
  \hspace{1cm}
  \input{figures/filter_dataset.pdftex_t}
  \caption{\texttt{job-1} separates the dataset
    \texttt{job-0/default} into two new datasets, named
    \texttt{job-1/train} and \texttt{job-1/test}.}
  \label{fig:dep_dataset_csvimport_chain}
\end{figure}

The figure shows how \texttt{job-1} has created two datasets,
\texttt{job-1/train} and \texttt{job-1/test}, based on the input
dataset \texttt{job-0/default}.  A third job, \texttt{job-2} is then
accessing the \texttt{job-1/train} dataset.  (Note that \texttt{job-1}
does not have a \texttt{default} dataset.)



%% Short repetition on the direction of the arrow.  \texttt{job\_1}
%% filtered the \texttt{dataset} from \texttt{job\_0}.  So if we take a
%% look at \texttt{job\_1} and wonder where the input data is, we just
%% follow the link back to \texttt{job\_0}.  If this job is an import, it
%% will hold the name of the file that was imported.  If not, it will
%% link back to another job, and so on, all the way down to the source of
%% the dataset.  So there is full tracking and easy observability of
%% everything that relates to job execution in the Accelerator.

\emph{
  Let us look at an example of when such dataset splitting
  makes sense, and how it relates to the design methodology that the
  Accelerator is based upon.  Assume that we have a (perhaps large)
  dataset that we want to split into, say, a training set and a
  validation set.  Even if one set is small, it makes sense to split the
  dataset into two disjoint sets.  This way, we ``physically'' separate
  the data into two sets, while keeping all the data in the same place.
  This is good for transparency reasons, and any method following the
  split may iterate over both subsets to read the complete data.
}


\subsection{Parallel Dataset Access and Hashing}
As shown in detail in section~\ref{chap:datasets}, data in datasets are stored in
multiple files, allowing for fast parallel reads.  The parameter
\texttt{slices}  determines how many slices that the dataset
should be partitioned into.  This parameter also sets the number of
parallel \texttt{analysis}-processes, so that each \texttt{analysis}
process operates on a unique slice of the dataset.

Datasets can be partitioned, sliced, in different ways.  One obvious
way is to use round robin, where each consecutive data row is written
to the next slice, modulo the number of slices.  This leads to
datasets with approximately equal number of rows per slice.  Another
alternative to slicing is to slice based on the hash value of a
particular column's values.  Using this method, all rows with the same
value in the hash column end up in the same slice.  This is efficient
for some parallel processing tasks.




\subsection{Dataset Column Types}
\label{sec:dataset-typing}
There are a number of useful types available for dataset columns.
They include floating and integer point numbers, booleans, timestamps,
several string types, and json types.  Several of these types are
designed to make importing data from text files straightforward,
without parse errors, overflows etc.



\subsection{Dataset Attributes}
The dataset has a number of attributes associated with it, such as
shape, number of rows, column names and types, and more.
An attribute is accessed like this
\begin{python}
datasets = ('source',)
def synthesis():
    print(datasets.source.shape)
\end{python}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Iterators: Working with Data}

Data in a dataset is typically accessed using an \emph{iterator} that
reads and streams one dataset slice at a time to a CPU core.  In this
section, we'll have a look at iterators for reading data, how to take
advantage of slicing to have parallel processing, and how to
efficiently create datasets.

\subsection{Iterator Basics}

Assume that we have a dataset with a column containing movie titles
named \texttt{movie}, and we want to know the ten most frequent
movies.  Consider the following example of a complete method
\begin{python}
from collections import Counter
datasets = ('source',)

def synthesis():
    c = Counter(datasets.source.iterate(None, 'movie'))
    print(c.most_common(10))
\end{python}
This will print the ten most common movie titles and their
corresponding counts in the \texttt{source} dataset.  The code will
run on a single CPU core, because of the \texttt{synthesis} function,
which is called only once.  The \texttt{iterate} (class-)method
therefore has to read through all slices, one at a time, in a serial
fashion, and this is reflected by the first argument to the iterator
being \texttt{None}.


\subsection{Parallel Execution}
The Accelerator is much about parallel processing, and since datasets
are sliced, we can modify the above program to execute in parallel by
doing the following modification
\begin{python}
def analysis(sliceno):
    return Counter(datasets.source.iterate(sliceno, 'movie'))

def synthesis(analysis_res)
    c = analysis_res.merge_auto()
    print(c.most_common(10))
\end{python}
Here, we run \texttt{iterate} inside the \texttt{analysis()} function.
This function is forked once for each slice, and the argument
\texttt{sliceno} will contain an integer between zero and the number
of slices minus one.  The returned value from the analysis functions
will be available as input to the synthesis function in the
\texttt{analysis\_res} Python iterable.  It is possible to merge the
results explicitly, but the iterator comes with a rather magic method
\texttt{merge\_auto()}, which merges the results from all slices into
one based on the data type.  It can for example merge
\texttt{Counter}s, \texttt{set}s, and composed types like
\texttt{set}s of \texttt{Counter}s, and so on.


\subsection{Iterating over Several Columns}
Since each column is stored independently in a dataset, there is no
overhead from reading a subset of a dataset's columns.  In the
previous section we've seen how to iterate over a single column using
\texttt{iterate}.  Iterating over more columns is straightforward by
feeding a list of column names to \texttt{iterate}, like in this
example
\begin{python}
from collections import defaultdict
datasets = {'source',}

def analysis(sliceno):
    user2movieset = defaultdict(set)
    for user, movie in datasets.source.iterate(sliceno, ('user', 'movie')):
    user2movieset[user].add(movie)
    return user2movieset
\end{python}
This example creates a lookup dictionary from users to sets of movies.
It is also possible to iterate over all columns by specifying an empty
list of columns or by using the value \texttt{None}.
\begin{python}
...
def analysis(sliceno):
    for columns in datasets.source.iterate(sliceno, None):
    print(columns)
    break
\end{python}
This example will print the first row for each slice of a dataset and
then exit.


\subsection{Iterating over Dataset Chains}

Previously, we've seen how to iterate over a single dataset using
\texttt{iterate}.  There is a corresponding function,
\texttt{iterate\_chain}, that is used for iterating over chains of
datasets.  This function takes a number of arguments, such as
\begin{itemize}
\item[] \texttt{length}, i.e.\ the number of datasets to iterate over.
  By default, it will iterate over all datasets in the chain.
\item[] \texttt{callbacks}, functions that can be called before and/or
  after each dataset in a chain.  Very useful for aggregating data
  between datasets.
\item[] \texttt{stop\_id} which stops iterating at a certain dataset.
  This dataset could be from \textsl{another} job's parameters, so we
  can for example iterate exactly over all new datasets not covered by
  a previous job.

\item[] \texttt{range}, which allows for iterating over a range of
  data.
\end{itemize}
The \texttt{range} options is based on the max/min values stored for
each column in the dataset.  Assuming that the chain is sorted, one
can for example set \mintinline{python}{range={timestamp, ('2016-01-01', '2016-01-31')}}
in order to get rows within the specified range only.  The %
\texttt{range} is quite costly, since it requires each row in the
dataset chain with dates within the range to be checked against the
range criterion.  Therefore, there is a \texttt{sloppy} version that
iterates over complete datasets in the chain that contains at least
one row with a date within the range.  This is useful, for example, to
very quickly produce histograms or plots of subsets of the data.


\subsection{Asserting the Hashlabel}
Depending on how the parallel processing is implemented in a method,
some methods will only work if the input datasets are hashed on a
certain column.  To make sure this is the case, there is an optional
\texttt{hashlabel} parameter to the iterators that will cause a
failure if the supplied column name does not correspond to the
dataset's hashlabel.



It is also possible to have the iterate re-hash on-the-fly.  In
general this is not recommended, since there is a
\texttt{dataset\_rehash} method that does the same and stores the
result for immediate re-use.  Using \texttt{dataset\_rehash} will be
much more efficient.

\subsection{Dataset Translators and Filters}

The iterator may perform data translation and filtering on-the-fly
using the \texttt{translators} and \texttt{filters} options.  Here is
an example of how a dictionary can be fed into the iterator to map a
column
\begin{python}
mapper = {2: "HUMANLIKE", 4: "LABRADOR", 5: "STARFISH",}
for animal in datasets.source.iterate_chain(sliceno, \
  "NUM_LEGS", translator={"NUM_LEGS": mapper,}):
    ...
\end{python}
This will iterate over the \texttt{NUM\_LEGS} column, and map numbers
to strings according to the \texttt{mapper} dict.

Filters work similarly.

